# Hauptkonfiguration für den Arzttarif Assistenten.
# Diese Datei bleibt manuell gepflegt und kommentiert.
# Laufzeitwerte (Fensterpositionen, Modellfaehigkeiten usw.) werden in config.runtime.ini gespeichert.

[RAG]
# 1 aktiviert Retrieval-Augmented-Generation mit Vektor-Embeddings, 0 deaktiviert den Modus.
enabled = 1

[APP]
# Anzeigetext für die Anwendungsversion (wird in GUI und API ausgegeben).
version = 4.8 (23.12.2025)
# Tarifstand, der in der Oberfläche kommuniziert wird.
tarif_version = Tarifversion 1.1c, Stand 28.11.2025

[FEATURES]
# 1 blendet den Link zum Brick-Quiz in der HTML-Oberfläche ein, 0 deaktiviert ihn.
brick_quiz_enabled = 1

[LLM1UND2]
# Anbieter für Stage 1 (z. B. gemini, openai, apertus, ollama).
stage1_provider = gemini
# Modellname für Stage 1 beim gewählten Anbieter.
stage1_model = gemini-2.5-flash
# stage1_model = gemini-3-flash-preview
# Temperatur für Stage 1 zur Kandidatenextraktion (0..1, niedriger = deterministischer).
stage1_temperature = 0.01
# Anbieter für Stage 2 (Ranking/Mapping der Vorschläge).
stage2_provider = gemini
# Modellname für Stage 2.
stage2_model = gemini-2.5-flash
# Temperatur für das Mapping in Stage 2 (0..1, beeinflusst Struktur).
stage2_mapping_temperature = 0.01
# Temperatur für das Ranking in Stage 2 (0..1, höher = mehr Varianz).
stage2_ranking_temperature = 0.1

[SYNONYMS]
# 1 aktiviert Synonym-Expansion in Backend und GUI, 0 deaktiviert sie.
enabled = 1
# Standarddatei für den Synonym-Katalog relativ zu data/.
catalog_filename = synonyms.json
# Anbieter für die Synonym-LLMs (z. B. gemini, openai, ollama).
llm_provider = gemini
# Modellname für die Synonym-Generierung.
llm_model = gemini-2.5-flash
# Temperatur für automatische Synonym-Vorschläge.
generation_temperature = 0.01
# Temperatur für Completion/Ranking-Schritte.
completion_temperature = 1.0
# Startfenster für die Listenansicht (Breite x Höhe + X + Y). Laufzeitwerte liegen in config.runtime.ini.
list_geometry = 1200x700+943+191
# Startspaltenbreiten der Listenansicht (LKN, Begriff, Synonyme, Status). Laufzeitwerte liegen in config.runtime.ini.
list_columns = 90,360,1200,20
# Startfenster für den Editor-Dialog (Breite x Höhe + X + Y).
edit_geometry = 600x400+208+208
# Startspaltenbreiten im Editor (Basisterm, Synonyme). Laufzeitwerte liegen in config.runtime.ini.
edit_columns = 184,404

[REGELPRUEFUNG]
# 1 verlangt explizite Kumulationsfreigaben, 0 erlaubt implizite Kombinationen.
kumulation_explizit = 0
# Kommagetrennte Service-Catalog-Tabellen, deren reine Treffer in der Erklärungsliste
# ausgeblendet werden (Groß-/Kleinschreibung wird ignoriert).
pauschale_explanation_excluded_lkn_tables = or, elt, nonelt, anast

[LOGGING]
# Schwellenwert für Konsolen-Logs (DEBUG, INFO, WARNING, ERROR, CRITICAL).
console_level = WARNING
# 1 aktiviert Datei-Logging ueber einen RotatingFileHandler.
file_enabled = 1
# Pfad zur Logdatei, wenn file_enabled = 1.
file_path = logs/app.log
# Maximalgroesse einer Logdatei in Bytes, bevor rotiert wird.
file_max_bytes = 10000000
# Anzahl der Log-Dateibackups, die behalten werden.
file_backup_count = 5
# Log-Level für den File-Handler (wirkt unabhaengig vom Konsolenlevel).
file_level = INFO
# 1 protokolliert die Roh-Eingaben an alle LLMs.
log_llm_input = 0
# 1 protokolliert den Benutzereingabetext für Rueckfragen.
log_input_text = 1
# 1 protokolliert die Prompttexte der LLM-Aufrufe.
log_llm_prompt = 0
# 1 protokolliert komplette LLM-Antworten.
log_llm_output = 0
# 1 protokolliert Tokenstatistiken je Aufruf.
log_tokens = 1
# 1 protokolliert das geparste JSON aus Stage 1.
log_s1_parsed_json = 0
# 1 speichert HTML-Renderings der Antwort zur Analyse.
log_html_output = 0

[LLM]
# Mindestabstand in Sekunden zwischen zwei LLM-Aufrufen (Drosselung).
min_call_interval_seconds = 2

[LLM_CAPABILITIES]
# Eintraege werden automatisch gesetzt (1 = Temperatur unterstuetzt, 0 = wird entfernt).
gpt-5-mini_supports_temperature = 0
gpt-5_supports_temperature = 0

[OPENAI]
# Timeout für HTTP-Anfragen in Sekunden.
timeout = 180
# Maximale Antwortlaenge für Standard-OpenAI-kompatible Modelle.
max_output_tokens = 20000
# Maximale Antwortlaenge für Apertus-kompatible Modelle.
max_output_tokens_apertus = 5000
# Tokenbudget für Default-Modelle (Prompt + Antwort).
token_budget_default = 100000
# Tokenbudget für Apertus-Modelle.
token_budget_apertus = 15000
# 1 aktiviert das Trimmen langer Prompts für Apertus, 0 deaktiviert.
trim_apertus_enabled = 0
# Wie oft hintereinander getrimmt werden darf.
trim_max_passes = 3
# Mindestanzahl an Zeichen, die trotz Trimming erhalten bleiben muessen.
trim_min_context_chars = 5000
# Anzahl Wiederholungen bei Serverfehlern (HTTP 5xx).
server_error_max_retries = 5
# Wartezeit in Sekunden zwischen Wiederholungen bei Serverfehlern.
server_error_retry_delay_seconds = 2

[GEMINI]
# Timeout für Gemini-Anfragen in Sekunden.
timeout = 120
# 1 aktiviert Token-Trimming für Gemini, 0 laesst den Prompt unveraendert.
trim_enabled = 0
# Prompt-Stil für Stage 1: auto (Flash => balanced), compact, balanced, full
prompt_style = full
# Maximale Antwortlänge (Tokens) für Stage 1 (kleiner = schneller, weniger Timeout-Risiko).
stage1_max_output_tokens = 8192
# Tokenbudget für Gemini-Modelle.
token_budget = 60000
# Mindestzeichenzahl, die trotz Trimming enthalten bleiben muss.
trim_min_context_chars = 30000
# Anzahl Wiederholungen bei Serverfehlern.
server_error_max_retries = 3
# Sekundenspezifischer Backoff zwischen Wiederholungen.
server_error_backoff_seconds = 1

[API_ENDPOINTS]
# Basis-URL für Apertus-kompatible OpenAI-APIs.
apertus_base_url = https://api.publicai.co/v1
# Basis-URL für das offizielle OpenAI-API.
openai_base_url = https://api.openai.com/v1

[CONTEXT]
# 1 fügt  die medizinische Interpretation hinzu, 0 spart Tokens und verändert kaum die Qualität.
include_med_interpretation = 0
# 1 übergibt den Tarif-Typ (z. B. AA, CA) an das LLM.
include_typ = 1
# 1 übergibt die Kurzbeschreibung der Leistung.
include_beschreibung = 1
# Anzahl "direkter Treffer" (Top-Kandidaten), die immer in den Stage‑1 Kontext übernommen werden.
direct_context_code_limit = 45
# Untere Token-Schwelle des Kontextblocks; fällt der Wert darunter, werden zusätzliche Interpretationen ergänzt (wenn vorhanden).
min_context_token_threshold = 9000
# Maximalzahl an Fallback-Zeilen mit medizinischer Interpretation.
max_fallback_med_lines = 80
# Synonym-/Varianten-Limits (höher = bessere Abdeckung, aber mehr Tokens/Noise).
max_query_variants = 140
max_token_variant_additions = 80
max_prompt_synonyms = 30
# Begrenzung für zusätzliche Kontextpositionen.
# 0 = nur direkte Treffer (spart Tokens, kann aber wichtige LKNs wie WA.* verdrängen).
max_context_items = 220
# Kommagetrennte Codes, die immer in den Kontext aufgenommen werden.
force_include_codes = AA.00.0010, CA.00.0010, AA.00.0020, CA.00.0020, CA.00.0030

[RENDER]
# 1 berechnet Regeldetails serverseitig für die HTML-Ausgabe.
server_side_conditions = 1
